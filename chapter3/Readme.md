# The Magical Tale of the Ever-Growing Library

<p align="center">
  <img src="https://raw.githubusercontent.com/olejardamir/hackingbtc/refs/heads/main/chapter3/chapter3.png?raw=true" width="500">
</p>

Once upon a time, in a land where knowledge shimmered like stars, there stood a grand library called the **Transformer Archives**. This library was not built of bricks and mortar, but of glowing scrolls filled with letters and numbers that changed and danced with every passing day.

At the heart of the Transformer Archives worked a devoted caretaker named **Modeller**. Each day, Modeller ventured out into the kingdom to gather new stories from travelers—recordings of great deeds, surprising failures, and everyday happenings. The travelers would write their tales onto a big scroll called the **Training Scroll**, which grew longer and heavier with each new entry. And it was Modeller’s job to make sure that no matter how large the scroll became, the library remained tidy and accessible to all.

---

## The Mysterious Magic of the GPT Scribe

Deep within the library stood a legendary scribe known as the **GPT Scribe**. This enchanted scribe had a special quill that could weave together words from the kingdom’s archives into brand-new stories. Yet the GPT Scribe always sought to learn more—because the more it learned, the more wondrous its stories became!

Every so often, Modeller would approach the GPT Scribe and whisper:

> “I have gathered fresh tales from the travelers. May your wisdom grow with these new words.”

Then the GPT Scribe would study the new entries, humming with excitement as it learned about far-off places and heroic deeds. Over time, the Scribe became wiser and more skilled at spinning grand stories that thrilled everyone in the land.

---

## The Watchful Eye of Pruning

But the Training Scroll was magical, too—it never stopped growing! If left unchecked, it could grow so big it would bury the entire library. That’s why Modeller also practiced the **Art of Pruning**. Like a careful gardener, Modeller trimmed the older or less useful pieces of the scroll to ensure it never became too large to handle. This way, the GPT Scribe could continue to learn without being overwhelmed by endless words.

---

## The Enchanted Spells of LoRA and Quant

Two secret spells helped keep the GPT Scribe nimble:

1. **LoRA**  
   A special charm that let the Scribe learn new ideas without having to remember everything from scratch. It was like carrying a small magical pouch for the newest treasures instead of dragging along a huge trunk.

2. **Quant**  
   This spell shrank the Scribe’s mighty scroll into a lighter form, making it easier to share stories far and wide. It did not remove any important ideas—just made them smaller, like folding a large map into a neat little square.

Whenever the Scribe felt overburdened, Modeller would invoke **LoRA** to lighten its load. And when travelers needed quick tales, the **Quant** spell made sure the Scribe’s stories arrived faster than ever.

---

## The Guardian of Locks and Keys

In this kingdom, knowledge was precious, and so were the Scribe’s secrets. To protect them, Modeller used **magical locks** whenever adding or removing pages from the Training Scroll or the library’s archives. These locks ensured that no two wizards tried to scribble on the same page at once. If they did, chaos might ensue and entire sections of the library could get lost!

---

## The Endless Quest for Knowledge

Day after day, Modeller continued this sacred cycle:

1. **Listen for new stories** from across the land.  
2. **Prune the scroll** if it grew too big.  
3. **Present fresh tales** to the GPT Scribe, who studied them eagerly.  
4. **Cast the LoRA and Quant spells** to keep the Scribe swift and strong.

Through countless days and nights, Modeller’s diligence kept the Transformer Archives flourishing. The GPT Scribe’s magical quill never tired, weaving grand new stories for the kingdom. And whenever the time was right, Modeller and the Scribe would lock the library doors, rest for a moment, and wake again to greet the next day’s wonders.

---

## References


1. **Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017).** Attention is all you need. In *Advances in Neural Information Processing Systems* (pp. 5998–6008).  
   [Link](https://papers.nips.cc/paper/7181-attention-is-all-you-need)

2. **Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020).** Language models are few-shot learners. *Advances in Neural Information Processing Systems, 33*, 1877–1901.  
   [Link](https://arxiv.org/abs/2005.14165)

3. **Cheng, Y., Wang, D., Zhou, P., & Zhang, T. (2018).** Model compression and acceleration for deep neural networks: The principles, progress, and challenges. *IEEE Signal Processing Magazine, 35*(1), 126–136. DOI: [10.1109/MSP.2017.2744880](https://doi.org/10.1109/MSP.2017.2744880)

4. **Hu, E., Shen, Y., Wallis, C., Allen-Zhu, Z., Li, Y., Wang, L., ... & Chen, W. (2021).** LoRA: Low-Rank Adaptation of Large Language Models. *arXiv preprint arXiv:2106.09685.*  
   [Link](https://arxiv.org/abs/2106.09685)

5. **Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., ... & Adam, H. (2018).** Quantization and training of neural networks for efficient integer-arithmetic-only inference. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition* (pp. 2704–2713).  
   [Link](https://doi.org/10.1109/CVPR.2018.00291)

6. **Niu, F., Recht, B., Re, C., & Wright, S. J. (2011).** Hogwild!: A lock-free approach to parallelizing stochastic gradient descent. In *Advances in Neural Information Processing Systems* (pp. 693–701).  
   [Link](https://papers.nips.cc/paper/2011/hash/4ac0e46b39bcae1d23d4c2e40d2bb45f-Abstract.html)

7. **Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S., & Shah, M. (2022).** Transformers in vision: A survey. *ACM Computing Surveys, 55*(5), 1–39. DOI: [10.1145/3536076](https://doi.org/10.1145/3536076)
